package github.com/hashicorp/terraform-provider-aws/internal/service/emr
import (
	"bytes"
	"context"
	"fmt"
	"log"
	"net/http"
	"os"
	"slices"
	"strings"
	"time"
	_ "unsafe" // Required for go:linkname

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/service/emr"
	awstypes "github.com/aws/aws-sdk-go-v2/service/emr/types"
	smithyjson "github.com/aws/smithy-go/encoding/json"
	"github.com/hashicorp/aws-sdk-go-base/v2/tfawserr"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	sdkretry "github.com/hashicorp/terraform-plugin-sdk/v2/helper/retry"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/structure"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"github.com/hashicorp/terraform-provider-aws/internal/conns"
	"github.com/hashicorp/terraform-provider-aws/internal/create"
	"github.com/hashicorp/terraform-provider-aws/internal/enum"
	"github.com/hashicorp/terraform-provider-aws/internal/errs"
	"github.com/hashicorp/terraform-provider-aws/internal/errs/sdkdiag"
	"github.com/hashicorp/terraform-provider-aws/internal/flex"
	tfjson "github.com/hashicorp/terraform-provider-aws/internal/json"
	"github.com/hashicorp/terraform-provider-aws/internal/retry"
	"github.com/hashicorp/terraform-provider-aws/internal/sdkv2"
	tfslices "github.com/hashicorp/terraform-provider-aws/internal/slices"
	tftags "github.com/hashicorp/terraform-provider-aws/internal/tags"
	"github.com/hashicorp/terraform-provider-aws/internal/tfresource"
	"github.com/hashicorp/terraform-provider-aws/internal/verify"
	"github.com/hashicorp/terraform-provider-aws/names"
)
func resourceClusterUpdate(ctx context.Context, d *schema.ResourceData, meta any) diag.Diagnostics {
	var diags diag.Diagnostics
	conn := meta.(*conns.AWSClient).EMRClient(ctx)

	if d.HasChange("visible_to_all_users") {
		input := &emr.SetVisibleToAllUsersInput{
			JobFlowIds:        []string{d.Id()},
			VisibleToAllUsers: aws.Bool(d.Get("visible_to_all_users").(bool)),
		}

		_, err := conn.SetVisibleToAllUsers(ctx, input)

		if err != nil {
			return sdkdiag.AppendErrorf(diags, "updating EMR Cluster (%s): setting visibility: %s", d.Id(), err)
		}
	}

	if d.HasChange("auto_termination_policy") {
		_, n := d.GetChange("auto_termination_policy")
		if len(n.([]any)) > 0 {
			input := &emr.PutAutoTerminationPolicyInput{
				AutoTerminationPolicy: expandAutoTerminationPolicy(n.([]any)),
				ClusterId:             aws.String(d.Id()),
			}

			_, err := conn.PutAutoTerminationPolicy(ctx, input)

			if err != nil {
				return sdkdiag.AppendErrorf(diags, "updating EMR Cluster (%s): setting auto termination policy: %s", d.Id(), err)
			}
		} else {
			input := &emr.RemoveAutoTerminationPolicyInput{
				ClusterId: aws.String(d.Id()),
			}

			_, err := conn.RemoveAutoTerminationPolicy(ctx, input)

			if err != nil {
				return sdkdiag.AppendErrorf(diags, "updating EMR Cluster (%s): removing auto termination policy: %s", d.Id(), err)
			}
		}
	}

	if d.HasChange("termination_protection") {
		input := &emr.SetTerminationProtectionInput{
			JobFlowIds:           []string{d.Id()},
			TerminationProtected: aws.Bool(d.Get("termination_protection").(bool)),
		}

		_, err := conn.SetTerminationProtection(ctx, input)

		if err != nil {
			return sdkdiag.AppendErrorf(diags, "updating EMR Cluster (%s): setting termination protection: %s", d.Id(), err)
		}
	}

	if d.HasChange("unhealthy_node_replacement") {
		input := &emr.SetUnhealthyNodeReplacementInput{
			JobFlowIds:               []string{d.Id()},
			UnhealthyNodeReplacement: aws.Bool(d.Get("unhealthy_node_replacement").(bool)),
		}

		_, err := conn.SetUnhealthyNodeReplacement(ctx, input)

		if err != nil {
			return sdkdiag.AppendErrorf(diags, "updating EMR Cluster (%s): setting unhealthy node replacement: %s", d.Id(), err)
		}
	}

	if d.HasChange("core_instance_group.0.autoscaling_policy") {
		autoscalingPolicyStr := d.Get("core_instance_group.0.autoscaling_policy").(string)
		instanceGroupID := d.Get("core_instance_group.0.id").(string)

		if autoscalingPolicyStr != "" {
			var autoScalingPolicy awstypes.AutoScalingPolicy

			if err := tfjson.DecodeFromString(autoscalingPolicyStr, &autoScalingPolicy); err != nil {
				return sdkdiag.AppendFromErr(diags, err)
			}

			input := &emr.PutAutoScalingPolicyInput{
				ClusterId:         aws.String(d.Id()),
				AutoScalingPolicy: &autoScalingPolicy,
				InstanceGroupId:   aws.String(instanceGroupID),
			}

			_, err := conn.PutAutoScalingPolicy(ctx, input)

			if err != nil {
				return sdkdiag.AppendErrorf(diags, "updating EMR Cluster (%s): setting autoscaling policy: %s", d.Id(), err)
			}
		} else {
			input := &emr.RemoveAutoScalingPolicyInput{
				ClusterId:       aws.String(d.Id()),
				InstanceGroupId: aws.String(instanceGroupID),
			}

			_, err := conn.RemoveAutoScalingPolicy(ctx, input)

			if err != nil {
				return sdkdiag.AppendErrorf(diags, "updating EMR Cluster (%s): removing autoscaling policy: %s", d.Id(), err)
			}

			// RemoveAutoScalingPolicy seems to have eventual consistency.
			// Retry reading Instance Group configuration until the policy is removed.
			const (
				timeout = 1 * time.Minute
			)
			_, err = tfresource.RetryUntilNotFound(ctx, timeout, func(ctx context.Context) (any, error) {
				return findCoreInstanceGroupAutoScalingPolicy(ctx, conn, d.Id())
			})

			if err != nil {
				return sdkdiag.AppendErrorf(diags, "updating EMR Cluster (%s): removing autoscaling policy: waiting for completion: %s", d.Id(), err)
			}
		}
	}

	if d.HasChange("core_instance_group.0.instance_count") {
		instanceGroupID := d.Get("core_instance_group.0.id").(string)

		input := &emr.ModifyInstanceGroupsInput{
			InstanceGroups: []awstypes.InstanceGroupModifyConfig{
				{
					InstanceGroupId: aws.String(instanceGroupID),
					InstanceCount:   aws.Int32(int32(d.Get("core_instance_group.0.instance_count").(int))),
				},
			},
		}

		_, err := conn.ModifyInstanceGroups(ctx, input)

		if err != nil {
			return sdkdiag.AppendErrorf(diags, "modifying EMR Cluster (%s) Instance Group (%s): %s", d.Id(), instanceGroupID, err)
		}

		const (
			timeout = 20 * time.Minute
		)
		if _, err := waitInstanceGroupRunning(ctx, conn, d.Id(), instanceGroupID, timeout); err != nil {
			return sdkdiag.AppendErrorf(diags, "waiting for EMR Cluster (%s) Instance Group (%s) modification: %s", d.Id(), instanceGroupID, err)
		}
	}

	if d.HasChange("instance_group") {
		o, n := d.GetChange("instance_group")
		oSet := o.(*schema.Set).List()
		nSet := n.(*schema.Set).List()
		for _, currInstanceGroup := range oSet {
			for _, nextInstanceGroup := range nSet {
				oInstanceGroup := currInstanceGroup.(map[string]any)
				nInstanceGroup := nextInstanceGroup.(map[string]any)

				if oInstanceGroup["instance_role"].(string) != nInstanceGroup["instance_role"].(string) || oInstanceGroup[names.AttrName].(string) != nInstanceGroup[names.AttrName].(string) {
					continue
				}

				// Prevent duplicate PutAutoScalingPolicy from earlier update logic
				if nInstanceGroup[names.AttrID] == d.Get("core_instance_group.0.id").(string) && d.HasChange("core_instance_group.0.autoscaling_policy") {
					continue
				}

				if v, ok := nInstanceGroup["autoscaling_policy"]; ok && v.(string) != "" {
					var autoScalingPolicy awstypes.AutoScalingPolicy

					if err := tfjson.DecodeFromString(v.(string), &autoScalingPolicy); err != nil {
						return sdkdiag.AppendFromErr(diags, err)
					}

					input := &emr.PutAutoScalingPolicyInput{
						ClusterId:         aws.String(d.Id()),
						AutoScalingPolicy: &autoScalingPolicy,
						InstanceGroupId:   aws.String(oInstanceGroup[names.AttrID].(string)),
					}

					_, err := conn.PutAutoScalingPolicy(ctx, input)

					if err != nil {
						return sdkdiag.AppendErrorf(diags, "updating autoscaling policy for instance group %q: %s", oInstanceGroup[names.AttrID].(string), err)
					}

					break
				}
			}
		}
	}

	if d.HasChange("step_concurrency_level") {
		input := &emr.ModifyClusterInput{
			ClusterId:            aws.String(d.Id()),
			StepConcurrencyLevel: aws.Int32(int32(d.Get("step_concurrency_level").(int))),
		}

		_, err := conn.ModifyCluster(ctx, input)

		if err != nil {
			return sdkdiag.AppendErrorf(diags, "updating EMR Cluster (%s): updating step concurrency level: %s", d.Id(), err)
		}
	}

	return append(diags, resourceClusterRead(ctx, d, meta)...)
}
