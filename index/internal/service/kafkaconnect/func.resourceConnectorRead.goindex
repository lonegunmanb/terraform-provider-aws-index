package github.com/hashicorp/terraform-provider-aws/internal/service/kafkaconnect
import (
	"context"
	"fmt"
	"log"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/service/kafkaconnect"
	awstypes "github.com/aws/aws-sdk-go-v2/service/kafkaconnect/types"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/retry"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"github.com/hashicorp/terraform-provider-aws/internal/conns"
	"github.com/hashicorp/terraform-provider-aws/internal/enum"
	"github.com/hashicorp/terraform-provider-aws/internal/errs"
	"github.com/hashicorp/terraform-provider-aws/internal/errs/sdkdiag"
	"github.com/hashicorp/terraform-provider-aws/internal/flex"
	tftags "github.com/hashicorp/terraform-provider-aws/internal/tags"
	"github.com/hashicorp/terraform-provider-aws/internal/tfresource"
	"github.com/hashicorp/terraform-provider-aws/internal/verify"
	"github.com/hashicorp/terraform-provider-aws/names"
)
func resourceConnectorRead(ctx context.Context, d *schema.ResourceData, meta any) diag.Diagnostics {
	var diags diag.Diagnostics
	conn := meta.(*conns.AWSClient).KafkaConnectClient(ctx)

	connector, err := findConnectorByARN(ctx, conn, d.Id())

	if tfresource.NotFound(err) && !d.IsNewResource() {
		log.Printf("[WARN] MSK Connect Connector (%s) not found, removing from state", d.Id())
		d.SetId("")
		return diags
	}

	if err != nil {
		return sdkdiag.AppendErrorf(diags, "reading MSK Connect Connector (%s): %s", d.Id(), err)
	}

	d.Set(names.AttrARN, connector.ConnectorArn)
	if connector.Capacity != nil {
		if err := d.Set("capacity", []any{flattenCapacityDescription(connector.Capacity)}); err != nil {
			return sdkdiag.AppendErrorf(diags, "setting capacity: %s", err)
		}
	} else {
		d.Set("capacity", nil)
	}
	d.Set("connector_configuration", connector.ConnectorConfiguration)
	d.Set(names.AttrDescription, connector.ConnectorDescription)
	if connector.KafkaCluster != nil {
		if err := d.Set("kafka_cluster", []any{flattenClusterDescription(connector.KafkaCluster)}); err != nil {
			return sdkdiag.AppendErrorf(diags, "setting kafka_cluster: %s", err)
		}
	} else {
		d.Set("kafka_cluster", nil)
	}
	if connector.KafkaClusterClientAuthentication != nil {
		if err := d.Set("kafka_cluster_client_authentication", []any{flattenClusterClientAuthenticationDescription(connector.KafkaClusterClientAuthentication)}); err != nil {
			return sdkdiag.AppendErrorf(diags, "setting kafka_cluster_client_authentication: %s", err)
		}
	} else {
		d.Set("kafka_cluster_client_authentication", nil)
	}
	if connector.KafkaClusterEncryptionInTransit != nil {
		if err := d.Set("kafka_cluster_encryption_in_transit", []any{flattenClusterEncryptionInTransitDescription(connector.KafkaClusterEncryptionInTransit)}); err != nil {
			return sdkdiag.AppendErrorf(diags, "setting kafka_cluster_encryption_in_transit: %s", err)
		}
	} else {
		d.Set("kafka_cluster_encryption_in_transit", nil)
	}
	d.Set("kafkaconnect_version", connector.KafkaConnectVersion)
	if connector.LogDelivery != nil {
		if err := d.Set("log_delivery", []any{flattenLogDeliveryDescription(connector.LogDelivery)}); err != nil {
			return sdkdiag.AppendErrorf(diags, "setting log_delivery: %s", err)
		}
	} else {
		d.Set("log_delivery", nil)
	}
	d.Set(names.AttrName, connector.ConnectorName)
	if err := d.Set("plugin", flattenPluginDescriptions(connector.Plugins)); err != nil {
		return sdkdiag.AppendErrorf(diags, "setting plugin: %s", err)
	}
	d.Set("service_execution_role_arn", connector.ServiceExecutionRoleArn)
	d.Set(names.AttrVersion, connector.CurrentVersion)
	if connector.WorkerConfiguration != nil {
		if err := d.Set("worker_configuration", []any{flattenWorkerConfigurationDescription(connector.WorkerConfiguration)}); err != nil {
			return sdkdiag.AppendErrorf(diags, "setting worker_configuration: %s", err)
		}
	} else {
		d.Set("worker_configuration", nil)
	}

	return diags
}
